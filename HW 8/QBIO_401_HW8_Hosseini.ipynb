{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08dd98f",
   "metadata": {},
   "source": [
    "# QBIO 401 Homework 8\n",
    "## Hirad Hosseini, Fall 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b26be",
   "metadata": {},
   "source": [
    "Assignment #8\n",
    "On Blackboard I have posted the data set wdbc.csv. In this assignment we are going to use\n",
    "logistic regression to try to predict the column “diagnosis” (M for malignant or B for benign).\n",
    "\n",
    "Each row is a different breast cancer patient. The first column is the ID_number of the patient\n",
    "and this column does not concern us. The second column is diagnosis and this is the Y-variable\n",
    "we are trying to predict. The next 30 columns are the numerical X-variables that we are going to\n",
    "use to try to predict diagnosis. (Note: there is no missing data, and there are no categorical X-\n",
    "variables so there is no need for “dummy” variables.) These 30 variables are computed from a\n",
    "fine-scale image of the breast mass. The first 10 of these variables are mean values from the\n",
    "cells. The next 10 of these variables are standard errors from the cells. And the last 10 of these\n",
    "variables are the “worst” (average of the three largest cells).\n",
    "\n",
    "Hint: See the code at the end of the notebook “regression2class1.ipynb” (in the “Regression 2\n",
    "Classification 1 folder) in the “Logistic Regression” section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3690f4",
   "metadata": {},
   "source": [
    "**Question 1: Randomly split the data into a training and test set (75% training and 25% set).\n",
    "Standardize the X-variables. Fit the logistic regression model on the training set.\n",
    "Calculate the fraction of times this model predicts the correct result on the test set.\n",
    "Compute the confusion matrix on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464e61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all dependencies for logistic regression model and confusion matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b86a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_number</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_number diagnosis  mean_radius  mean_texture  mean_perimeter  \\\n",
       "0       842302         M        17.99         10.38          122.80   \n",
       "1       842517         M        20.57         17.77          132.90   \n",
       "2     84300903         M        19.69         21.25          130.00   \n",
       "3     84348301         M        11.42         20.38           77.58   \n",
       "4     84358402         M        20.29         14.34          135.10   \n",
       "..         ...       ...          ...           ...             ...   \n",
       "564     926424         M        21.56         22.39          142.00   \n",
       "565     926682         M        20.13         28.25          131.20   \n",
       "566     926954         M        16.60         28.08          108.30   \n",
       "567     927241         M        20.60         29.33          140.10   \n",
       "568      92751         B         7.76         24.54           47.92   \n",
       "\n",
       "     mean_area  mean_smoothness  mean_compactness  mean_concavity  \\\n",
       "0       1001.0          0.11840           0.27760         0.30010   \n",
       "1       1326.0          0.08474           0.07864         0.08690   \n",
       "2       1203.0          0.10960           0.15990         0.19740   \n",
       "3        386.1          0.14250           0.28390         0.24140   \n",
       "4       1297.0          0.10030           0.13280         0.19800   \n",
       "..         ...              ...               ...             ...   \n",
       "564     1479.0          0.11100           0.11590         0.24390   \n",
       "565     1261.0          0.09780           0.10340         0.14400   \n",
       "566      858.1          0.08455           0.10230         0.09251   \n",
       "567     1265.0          0.11780           0.27700         0.35140   \n",
       "568      181.0          0.05263           0.04362         0.00000   \n",
       "\n",
       "     mean_concave_points  ...  worst_radius  worst_texture  worst_perimeter  \\\n",
       "0                0.14710  ...        25.380          17.33           184.60   \n",
       "1                0.07017  ...        24.990          23.41           158.80   \n",
       "2                0.12790  ...        23.570          25.53           152.50   \n",
       "3                0.10520  ...        14.910          26.50            98.87   \n",
       "4                0.10430  ...        22.540          16.67           152.20   \n",
       "..                   ...  ...           ...            ...              ...   \n",
       "564              0.13890  ...        25.450          26.40           166.10   \n",
       "565              0.09791  ...        23.690          38.25           155.00   \n",
       "566              0.05302  ...        18.980          34.12           126.70   \n",
       "567              0.15200  ...        25.740          39.42           184.60   \n",
       "568              0.00000  ...         9.456          30.37            59.16   \n",
       "\n",
       "     worst_area  worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst_concave_points  worst_symmetry  worst_fractal_dimension  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wdbc.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee3cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"diagnosis\"] #feature to be predicted\n",
    "temp_X = df.drop(\"diagnosis\", axis=1) #all other features to be trained on \n",
    "X = pd.get_dummies(temp_X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db371065",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100) #Random state of 100 \n",
    "#for consistent output\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77cf8183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e2ea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.93066065e-06, 9.99997069e-01],\n",
       "       [6.72685921e-01, 3.27314079e-01],\n",
       "       [6.19065613e-04, 9.99380934e-01],\n",
       "       [9.99987712e-01, 1.22877755e-05],\n",
       "       [9.97021098e-01, 2.97890240e-03],\n",
       "       [9.74417970e-01, 2.55820301e-02],\n",
       "       [3.33579966e-01, 6.66420034e-01],\n",
       "       [1.03968055e-06, 9.99998960e-01],\n",
       "       [9.50715099e-01, 4.92849012e-02],\n",
       "       [9.25352228e-01, 7.46477717e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict_proba(X_test[0:10]) #Predictions appear to be strongly in favor of one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef66ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B', 'M', 'B', 'B', 'B', 'M', 'M', 'B', 'B'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict(X_test[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c390d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400    M\n",
       "225    B\n",
       "321    M\n",
       "173    B\n",
       "506    B\n",
       "380    B\n",
       "197    M\n",
       "260    M\n",
       "40     M\n",
       "160    B\n",
       "Name: diagnosis, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10] #Predictions and ground truth mostly match for first 10 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2fb4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test == log_reg.predict(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaaf13d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87,  0],\n",
       "       [ 5, 51]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, log_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03d64",
   "metadata": {},
   "source": [
    "This model predicts the correct diagnosis label approximatley 96.5% of the time for the test set. The confusion matrix demonstrates a high percent of true positives and true negatives, with a few false negatives. Overall, this logistic regression model is a good rudimentary start for predicting diagnoses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b920f",
   "metadata": {},
   "source": [
    "**Question 2: Next we are going to see how much our result in #1 depends on the initial random split\n",
    "into a training and test set. Use a loop to repeat the following 100 times: randomly split\n",
    "the data into a training and test split (75% training and 25% set), standardize the X-\n",
    "variables, fit the logistic regression model on the training set, and calculate the fraction\n",
    "of times this model predicts the correct result on the test set. (Note: you do not need to\n",
    "compute the confusion matrix for this problem.) Once you have the 100 estimates of\n",
    "the fraction of times the model predicts the correct result on the test set, report the\n",
    "95% bootstrap confidence interval for this estimate (sort the 100 estimates, and report\n",
    "the values at position 1 and 98).**\n",
    "\n",
    "Hint: it is important that each split into training and test sets is different. If you don’t\n",
    "specify “random_state” (see below), it will be different every time:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8746e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95% bootstrap confidence interval for the estimate fraction of correct predictions is [0.9440559440559441, 0.993006993006993]\n"
     ]
    }
   ],
   "source": [
    "frac_correct_pred_estims = [] #contains all model accuracy scores for 100 trials\n",
    "for trial in range(100): #conduct 100 trials for bootsrapping\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    scaler = StandardScaler().fit(X_train) #standardizes X variables\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train, y_train) #fits logistic regression model to the data\n",
    "    frac_correct_pred_estims.append(np.mean(y_test == log_reg.predict(X_test))) #obtains accuracy score for model\n",
    "frac_correct_pred_estims.sort()\n",
    "print(f\"The 95% bootstrap confidence interval for the estimate fraction of correct predictions is [{frac_correct_pred_estims[1]}, {frac_correct_pred_estims[98]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb3d93",
   "metadata": {},
   "source": [
    "Our obtained 95% confidence interval of approximately (0.944, 0.993) demonstrates that a training-test split of 75/25% is a generally good baseline for ensuring model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a27145",
   "metadata": {},
   "source": [
    "**Question 3: Next we are going to see how much our result in #2 depends on the fraction of data in\n",
    "the training and test split. Repeat #2 but now every time through the loop split the data\n",
    "so that only 10% is in the training set and 90% is in the test set. How does the\n",
    "confidence interval for #3 compare to the confidence interval for #2?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb5ef362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95% bootstrap confidence interval for the estimate fraction of correct predictions is [0.9220272904483431, 0.9727095516569201]\n"
     ]
    }
   ],
   "source": [
    "frac_correct_pred_estims = []\n",
    "for trial in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    frac_correct_pred_estims.append(np.mean(y_test == log_reg.predict(X_test)))\n",
    "frac_correct_pred_estims.sort()\n",
    "print(f\"The 95% bootstrap confidence interval for the estimate fraction of correct predictions is [{frac_correct_pred_estims[1]}, {frac_correct_pred_estims[98]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b1da2",
   "metadata": {},
   "source": [
    "Our obtained 95% confidence interval of approximately (0.922, 0.973) demonstrates that a training-test split of 10/90% is slightly worse for model accuracy than the 75/25 split (from Task 2), which may indicate that we should allocate more of our data for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb995e",
   "metadata": {},
   "source": [
    "**Question 4: Finally, we are going to test the importance of standardizing the data. Repeat #2 (so\n",
    "back to 75% training and 25% test) but never standardize the X-variables. You will have\n",
    "to make an additional change to the code. If not, for at least some of the times through\n",
    "the loop you will get an error that the model did not converge (already showing that not\n",
    "standardizing the X-variables is a problem). The default number of iterations for logistic\n",
    "regression is 100, the code below shows how to set it to be 5,000 (which will be\n",
    "sufficient for this problem, note: this will make the code take longer to run, the entire\n",
    "loop of 100 should take 1 or 2 minutes):**\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "\n",
    "**How does the confidence interval for #4 compare to the confidence interval for #2?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2eba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95% bootstrap confidence interval for the estimate fraction of correct predictions is [0.3146853146853147, 0.6923076923076923]\n"
     ]
    }
   ],
   "source": [
    "frac_correct_pred_estims = []\n",
    "for trial in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    log_reg = LogisticRegression(max_iter=5000) #maximizes number of iterations for logreg model to prevent freezing\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    frac_correct_pred_estims.append(np.mean(y_test == log_reg.predict(X_test)))\n",
    "frac_correct_pred_estims.sort()\n",
    "print(f\"The 95% bootstrap confidence interval for the estimate fraction of correct predictions is [{frac_correct_pred_estims[1]}, {frac_correct_pred_estims[98]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7739f0",
   "metadata": {},
   "source": [
    "Our obtained 95% confidence interval of approximately (0.315, 0.692) demonstrates that standardizing the X-variables is very important for enhancing model accuracy because our observed accuracy confidence interval is far lower than that of Task 2.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (rna)",
   "language": "python",
   "name": "rna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
